{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs English!!! For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"English!!! For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"what-is-mocapforall/","text":"What is MocapForAll? Motion capture software for everyone No special equipment required You can capture human motion if you have the followings: a middle range PC 2 or more webcams a room of about 2.5m x 2.5m You can use regular webcams like the ones used in video conferences. You can also use apps that turns your smartphones or tablets into webcams. Realtime with a middle range PC For examples, it runs at around 17 fps on Surface Pro 7 which does not have a dedicated GPU 30 to 60 fps on GTX 1080 Ti Cheap An option for projects with limited budgets, such as indie games, indie films, or education. What you can do with MocapForAll You can output captured motion to the network via VMT protocol(*1) and VMC protocol in real time(*2). You can save the captured motion to files in BVH format. You can output captured motion to the shared memory in real time. (*1 The \"VMT protocol\" here refers to the message format used in the communication of Virtual Motion Tracker . The official HP of Virtual Motion Tracker does not use the word \"VMT protocol\", but MocapForAll uses the word \"VMT protocol\" for convenience.) (*2 Both VMT protocol and VMC protocol use UDP/OpenSound Control.) By using these, you can do the followings, as well as you can even create your own programs to receive data since the output specifications are open to public. Use in SteamVR via Virtual Motion Tracker Through Virtual Motion Tracker , the capture motion can be used as virtual trackers in applications running on SteamVR. Use in Unreal Engine4, Unreal Engine5, Unity You can send the captured motion to Unreal Engine4, Unreal Engine5, or Unity for game development or video production. Plugins for linking data directly to UE4, UE5, or Unity are available here . As described in Use in other apps via VMC protocol , it is also possible to link by EVMC4U and VMC4UE using VMC Protocol. A Unity sample to read data from the shared memory written by MocapForAll is available here . Use in other apps via VMC protocol You can send the captured motion to various applications via the VMC protocol. The following are confirmed to work: Sending tracker to VirtualMotionCapture Sending bones to VSeeFace , and receiving facial expression morphs from VSeeFace Sending bones and facial expression morphs to EVMC4U Sending bones and facial expression morphs to VMC4UE Sending bones to VMC4B Export in BVH format You can save the captured motion as BVH files. It can be used with Blender etc.","title":"What is MocapForAll?"},{"location":"what-is-mocapforall/#what-is-mocapforall","text":"","title":"What is MocapForAll?"},{"location":"what-is-mocapforall/#motion-capture-software-for-everyone","text":"","title":"Motion capture software for everyone"},{"location":"what-is-mocapforall/#no-special-equipment-required","text":"You can capture human motion if you have the followings: a middle range PC 2 or more webcams a room of about 2.5m x 2.5m You can use regular webcams like the ones used in video conferences. You can also use apps that turns your smartphones or tablets into webcams.","title":"No special equipment required"},{"location":"what-is-mocapforall/#realtime-with-a-middle-range-pc","text":"For examples, it runs at around 17 fps on Surface Pro 7 which does not have a dedicated GPU 30 to 60 fps on GTX 1080 Ti","title":"Realtime with a middle range PC"},{"location":"what-is-mocapforall/#cheap","text":"An option for projects with limited budgets, such as indie games, indie films, or education.","title":"Cheap"},{"location":"what-is-mocapforall/#what-you-can-do-with-mocapforall","text":"You can output captured motion to the network via VMT protocol(*1) and VMC protocol in real time(*2). You can save the captured motion to files in BVH format. You can output captured motion to the shared memory in real time. (*1 The \"VMT protocol\" here refers to the message format used in the communication of Virtual Motion Tracker . The official HP of Virtual Motion Tracker does not use the word \"VMT protocol\", but MocapForAll uses the word \"VMT protocol\" for convenience.) (*2 Both VMT protocol and VMC protocol use UDP/OpenSound Control.) By using these, you can do the followings, as well as you can even create your own programs to receive data since the output specifications are open to public.","title":"What you can do with MocapForAll"},{"location":"what-is-mocapforall/#use-in-steamvr-via-virtual-motion-tracker","text":"Through Virtual Motion Tracker , the capture motion can be used as virtual trackers in applications running on SteamVR.","title":"Use in SteamVR via Virtual Motion Tracker"},{"location":"what-is-mocapforall/#use-in-unreal-engine4-unreal-engine5-unity","text":"You can send the captured motion to Unreal Engine4, Unreal Engine5, or Unity for game development or video production. Plugins for linking data directly to UE4, UE5, or Unity are available here . As described in Use in other apps via VMC protocol , it is also possible to link by EVMC4U and VMC4UE using VMC Protocol. A Unity sample to read data from the shared memory written by MocapForAll is available here .","title":"Use in Unreal Engine4, Unreal Engine5, Unity"},{"location":"what-is-mocapforall/#use-in-other-apps-via-vmc-protocol","text":"You can send the captured motion to various applications via the VMC protocol. The following are confirmed to work: Sending tracker to VirtualMotionCapture Sending bones to VSeeFace , and receiving facial expression morphs from VSeeFace Sending bones and facial expression morphs to EVMC4U Sending bones and facial expression morphs to VMC4UE Sending bones to VMC4B","title":"Use in other apps via VMC protocol"},{"location":"what-is-mocapforall/#export-in-bvh-format","text":"You can save the captured motion as BVH files. It can be used with Blender etc.","title":"Export in BVH format"},{"location":"how-to-capture/","text":"","title":"How to capture motion"},{"location":"how-to-capture/calibrate-cameras/","text":"Calibrate cameras {: .no_toc } Contents {: .no_toc .text-delta } TOC What is camera calibration? Camera calibration is the process of obtaining information about the relationship between the positions in the camera image and the positions in the real world. This section describes the concept of what you are doing with camera calibration and some tips for it. In camera calibration in MocapForAll, the following 2 types of information are obtained: Intrinsic parameters (Characteristics of the camera itself) Extrinsic parameters (Position of the camera in the real world) What is intrinsic parameters? Intrinsic parameters are the focal length f of the lens and the position of the optical axis Cx, Cy , which describes the characteristics of the camera itself . These are unique to the camera (lens) and basically do not change. Therefore, once you get them correctly, you don't need to get them again. Mathematically, it is represented by a matrix that describes the relationship between \"positions in the camera image\" and \"positions in the camera coordinate\". This is the matrix displayed on the app screen. Notes on autofocus {: .no_toc } If you use a camera with autofocus, keep in mind that as the focus changes, the focal length changes, so the intrinsic parameter also changes. In our experiences, it does not cause much problems when using regular webcams or mobile phones, but if high accuracy is not obtained, you may be need to disable autofocus. What is extrinsic parameters? Extrinsic parameters are the position and the orientation of the camera in the real world . In theory, once the camera is completely fixed in your room, it can be treated as a fixed value. But in practice, it sometimes shifts little by little over time. So, it is recommended to obtain it again every time you start using MocapForAll . Mathematically, it is represented by a matrix that describes the relationship between \"positions in the camera coordinate\" and \"positions in the world coordinate\". This is the matrix displayed on the app screen. 4 methods to get extrinsic parameters In MocapForAll, there are 4 methods to get extrinsic parameters. Please note that the preparation and execution procedures are different for each. Method Accuracy Ease of preparation The size of the usable space \u3000Comment 1. Method using ChArUco board \ud83d\udc4d\ud83d\udc4d\ud83d\udc4d \ud83d\udc4d \ud83d\udc4d The most accurate, but requires a little work to prepare. I think this is the easiest way in the long run. 2. Method using ArUco cluster \ud83d\udc4d\ud83d\udc4d \ud83d\udc4d\ud83d\udc4d \ud83d\udc4d\ud83d\udc4d This is a method with good accuracy and easy preparation. For beginners, I recommend that you try this method first. 3. Method using Diamond cluster \ud83d\udc4d\ud83d\udc4d\ud83d\udc4d \ud83d\udc80 \ud83d\udc4d\ud83d\udc4d\ud83d\udc4d This method allows capturing in a large space with many cameras by measuring the relative positions of multiple markers, though It takes time to prepare. 4. Method using human motion \ud83d\udc4d \ud83d\udc4d\ud83d\udc4d\ud83d\udc4d \ud83d\udc4d\ud83d\udc4d This method allows capturing in an environment where the marker cannot be printed or placed (for example, outdoors). Preparation for camera calibration 1: Print AR markers In order to calibrate a camera, it is necessary to \"find the positions of points in the camera image whose positions in the real world are known\". To do that, you need to take pictures of the \"specific images\" described below with the camera that actually you are using, so that the application can calculate the before-mentioned camera information from the pictures. In this section, we will print the \"specific images\" in preparation for camera calibration. Print the image for intrinsic parameter calibration We will use this image . If you haven't fixed the camera in the room yet, you don't need to print it out. We will use the above image by showing on the PC display. If you have already fixed the camera, print the above image in A4 size. The size does not have to be exact. Then tape it to a cardboard box to keep it flat. Print the image for extrinsic parameter calibration The image differs depending on the 4 methods to get extrinsic parameters explained before. Either way, the size does not have to be exact. Method using ChArUco board We will use this image . Print this in A2 or larger . You don't have a printer which can print A2? (Me too) Then, it is recommended to divide the image into two pieces, print them on two sheets of A3 paper, and tape them together. If you tape it on a cardboard like this so that it keeps in a clean flat state, you can continue to use it for a long time. \uff08The one in the photo above has been used for about 3 months, but it is still good to use.) Method using ArUco cluster We will use \"arucoMarker0.png\", \"(same)1.png\" and \"(same)2.png\" in this zip . Print them in A4 or A3. A4 is enough for a room that is not very large. Method using Diamond cluster We will use \"diamondMarker0.png\" and others in this zip . Print them in A2 or larger in the same way as \"1. Method using ChArUco board\". Method using human motion There is nothing to print. Preparation for camera calibration 2: Measure the marker size You need to measure the size of the actual printed image for extrinsic parameter calibration. The measured values will be used to define the scale of the captured movement. The part to measure differs depending on the 4 methods to get extrinsic parameters explained before. Method using ChArUco board Put the value in \"Settings > Calibration > Maker size (affects to coord. scale) > ChArUco board [m]\". The unit is meters. Method using ArUco cluster Put the value in \"Settings > Calibration > Maker size (affects to coord. scale) > ArUco marker [m]\". The unit is meters. Method using Diamond cluster Put the value in \"Settings > Calibration > Maker size (affects to coord. scale) > Diamond marker [m]\". The unit is meters. Method using human motion Measure the height of your body. Put the value in \"Settings > Calibration > Maker size (affects to coord. scale) > Human hight [m]\". The unit is meters. Connect cameras Connect at least 2 cameras to your PC. Click \"Add camera\" button at the top of the MocapForAll window. Select the combo box next to \"Camera:\" to find the connected camera. \u3000 You can change the image size of the camera by entering the dimensions and clicking \"Apply\" if camera supports the specified image size. Somtimes it fails to change the image size. In that case, please close camera, wait for a moment, and try again. You can flip the image horizontally. Note that some cameras have a mirror image by default. If the image is a mirror image, the AR marker cannot be read. You can rotate the image. At the start of capture, only a person standing upright can be recognized. So, rotate the image appropriately according to the actual orientation of the camera. Select camera control framework You can select the framework to control the camera by pressing the \"\u25bc\" next to \"Add camera\" at the top of the MocapForAll window. Direct show: Microsoft's media framework. You can use the OBS-VirtualCam plugin with this. UE4 media player: UE4's media framework. Better performance at high resolution. Some cameras don't work with this. Recorded video : Use recorded video instead of camera. See Motion capture from recorded videos . If the camera works with UE4 media player, it is recommended to use it. If it doesn't work, use Direct Show. Calibrate intrinsic parameters If you have not printed the image in Print the image for intrinsic parameter calibration section, display this image on your PC's screen as large as possible. In MocapForAll, click \"Start\" button under \"Camera > Calibration > Intrinsic\". Take images with the camera from various angles for about 10 seconds. If the camera is already fixed, move the image instead of the camera. When calibration is completed, intrinsic parameters will be displayed on the app's screen and \"Intrinsic \u2611Calibrated\" will be shown. \u3000 Save the camera calibration result from \"Save\". Calibrate extrinsic parameters The procedure differs depending on the 4 methods to get extrinsic parameters explained before. Method using ChArUco board Place the image printed in Print the image for extrinsic parameter calibration section on the floor. This will be the origin of the captured motion. Place the cameras where they can see the placed image. In MocapForAll, select \" ChArUco board (default) \" in \"Settings > Calibration > Extrinsic calibration method\". Click \"Start\" button under \"Camera > Calibration > Extrinsic\". When calibration is completed, extrinsic parameters and the camera position will be displayed, and it will shows \"Extrinsic \u2611Calibrated\". If it does not read the AR marker properly, try If the marker cannot be read properly . \u3000 Method using ArUco cluster Place the images printed in Print the image for extrinsic parameter calibration section on the floor. \"arucoMarker0.png\" will be the origin of the captured motion. Place the cameras where they can see the placed images. In MocapForAll, select \" ArUco cluster \" in \"Settings > Calibration > Extrinsic calibration method\". Click \" Scan markers \" button under \"Camera > Calibration > Extrinsic\". This will scan the relative positions of the markers. After a while, the markers will appear in the 3D viewport. After all markers appeared, click \" Stop scanning \" and then \" Start \" under \"Camera > Calibration > Extrinsic\". When calibration is completed, extrinsic parameters and the camera position will be displayed, and it will shows \"Extrinsic \u2611Calibrated\". If it does not read the AR marker properly, try If the marker cannot be read properly . \u3000 Method using Diamond cluster Place the images printed in Print the image for extrinsic parameter calibration section at a distance that fits in a frame of the camera. It does not have to be in the same plane. \"diamondMarker0.png\" will be the origin of the captured motion, so place this on the floor. In MocapForAll, select \" Diamond cluster \" in \"Settings > Calibration > Extrinsic calibration method\". Click \" Scan markers \" button under \"Camera > Calibration > Extrinsic\" in one of the cameras. Take pictures of \"diamondMarker0.png\" and one of the markers at the same time. After a while, the position of another marker will be fixed with \"diamondMarker0.png\" as the origin, and the marker will be displayed on the 3D viewport. Repeat the above step for markers whose positions have been fixed and markers whose positions have not been fixed, and click \"Stop scanning\" when the positions of all markers have been fixed. Place the cameras where they can see at least one of the markers whose position is fixed. Click \" Start \" under \"Camera > Calibration > Extrinsic\". When calibration is completed, extrinsic parameters and the camera position will be displayed, and it will shows \"Extrinsic \u2611Calibrated\". \u3000 Method using human motion Place the cameras so that at least two of them can see your whole body at the same time. In MocapForAll, select \" Human motion \" in \"Settings > Calibration > Extrinsic calibration method\". Select \" GPU_DirectML \" in \"Settings > General > Run DNN on\" if your hardware support it. (If you have already installed the \"GPU_TensorRT\" mode in the Appendix, you can also use it.\uff09 Select \" Speed \" in \"Settings > General > Capture body\" if your PC's performance is enough for it. (If you have already installed the \"Precision\" mode in the Appendix, you can also use it.\uff09 Click \"Start calibration\" button at the top of the window , and walk around on the floor where the cameras can see your whole body. Your motion will be captured and the positions of your joints will be used to find the cameras relative positions. After all cameras' extrinsic parameters calibrated, click the \" Find Ground \" button at the top of the window and walk around in the same way as If the marker cannot be read properly described below. This will determine the absolute position of the cameras. \u3000 If the marker cannot be read properly Since the position of the marker will be the plane with zero height of the captured motion, it is basically recommended to place the marker on the floor, but it may be difficult to read the marker on the floor depending on the placement and specification of the camera. In that case, you can hang the marker on the wall to calibrate the camera and adjust the floor level later . Procedure: - Replace the word \"on the floor\" with \"on the wall\" and execute the calibration procedure. - Select \" GPU_DirectML \" in \"Settings > General > Run DNN on\" if your hardware support it. (If you have already installed the \"GPU_TensorRT\" mode in the Appendix, you can also use it.\uff09 Select \" Speed \" in \"Settings > General > Capture body\" if your PC's performance is enough for it. (If you have already installed the \"Precision\" mode in the Appendix, you can also use it.\uff09 - Click the \" Find Ground \" button at the top of the window and walk around where at least 2 cameras can see your whole body. After a while, the position of the floor level will be adjusted automatically. \u3000 Confirm the calibration results If you have input the correct value in Preparation for camera calibration 2: Measure the marker size , you can see the position of the camera in the 3D viewport. For how to move the viewpoint, refer to Move viewport . Note that the camera may appear below the floor (in this case, the calibration has failed). Save and load the calibration results After calibration is completed, save the calibration result of each camera by clicking \" Save \" under \"Camera > Calibration > Intrinsic\" and \"Extrinsic\". The saved result can be loaded by clicking \" Load \" button. Also, you can save and load the whole camera setup by clicking \" Save All Cameras \" and \" Load All Cameras \" buttons. Note that the camera selection is saved as an index inside the combo box. If you remove the cameras from your PC, the order of the cameras in the combo box will change and you will not be able to load the cameras properly.","title":"Calibrate cameras"},{"location":"how-to-capture/calibrate-cameras/#calibrate-cameras","text":"{: .no_toc }","title":"Calibrate cameras"},{"location":"how-to-capture/calibrate-cameras/#contents","text":"{: .no_toc .text-delta } TOC","title":"Contents"},{"location":"how-to-capture/calibrate-cameras/#what-is-camera-calibration","text":"Camera calibration is the process of obtaining information about the relationship between the positions in the camera image and the positions in the real world. This section describes the concept of what you are doing with camera calibration and some tips for it. In camera calibration in MocapForAll, the following 2 types of information are obtained: Intrinsic parameters (Characteristics of the camera itself) Extrinsic parameters (Position of the camera in the real world)","title":"What is camera calibration?"},{"location":"how-to-capture/calibrate-cameras/#what-is-intrinsic-parameters","text":"Intrinsic parameters are the focal length f of the lens and the position of the optical axis Cx, Cy , which describes the characteristics of the camera itself . These are unique to the camera (lens) and basically do not change. Therefore, once you get them correctly, you don't need to get them again. Mathematically, it is represented by a matrix that describes the relationship between \"positions in the camera image\" and \"positions in the camera coordinate\". This is the matrix displayed on the app screen.","title":"What is intrinsic parameters?"},{"location":"how-to-capture/calibrate-cameras/#notes-on-autofocus","text":"{: .no_toc } If you use a camera with autofocus, keep in mind that as the focus changes, the focal length changes, so the intrinsic parameter also changes. In our experiences, it does not cause much problems when using regular webcams or mobile phones, but if high accuracy is not obtained, you may be need to disable autofocus.","title":"Notes on autofocus"},{"location":"how-to-capture/calibrate-cameras/#what-is-extrinsic-parameters","text":"Extrinsic parameters are the position and the orientation of the camera in the real world . In theory, once the camera is completely fixed in your room, it can be treated as a fixed value. But in practice, it sometimes shifts little by little over time. So, it is recommended to obtain it again every time you start using MocapForAll . Mathematically, it is represented by a matrix that describes the relationship between \"positions in the camera coordinate\" and \"positions in the world coordinate\". This is the matrix displayed on the app screen.","title":"What is extrinsic parameters?"},{"location":"how-to-capture/calibrate-cameras/#4-methods-to-get-extrinsic-parameters","text":"In MocapForAll, there are 4 methods to get extrinsic parameters. Please note that the preparation and execution procedures are different for each. Method Accuracy Ease of preparation The size of the usable space \u3000Comment 1. Method using ChArUco board \ud83d\udc4d\ud83d\udc4d\ud83d\udc4d \ud83d\udc4d \ud83d\udc4d The most accurate, but requires a little work to prepare. I think this is the easiest way in the long run. 2. Method using ArUco cluster \ud83d\udc4d\ud83d\udc4d \ud83d\udc4d\ud83d\udc4d \ud83d\udc4d\ud83d\udc4d This is a method with good accuracy and easy preparation. For beginners, I recommend that you try this method first. 3. Method using Diamond cluster \ud83d\udc4d\ud83d\udc4d\ud83d\udc4d \ud83d\udc80 \ud83d\udc4d\ud83d\udc4d\ud83d\udc4d This method allows capturing in a large space with many cameras by measuring the relative positions of multiple markers, though It takes time to prepare. 4. Method using human motion \ud83d\udc4d \ud83d\udc4d\ud83d\udc4d\ud83d\udc4d \ud83d\udc4d\ud83d\udc4d This method allows capturing in an environment where the marker cannot be printed or placed (for example, outdoors).","title":"4 methods to get extrinsic parameters"},{"location":"how-to-capture/calibrate-cameras/#preparation-for-camera-calibration-1-print-ar-markers","text":"In order to calibrate a camera, it is necessary to \"find the positions of points in the camera image whose positions in the real world are known\". To do that, you need to take pictures of the \"specific images\" described below with the camera that actually you are using, so that the application can calculate the before-mentioned camera information from the pictures. In this section, we will print the \"specific images\" in preparation for camera calibration.","title":"Preparation for camera calibration 1: Print AR markers"},{"location":"how-to-capture/calibrate-cameras/#print-the-image-for-intrinsic-parameter-calibration","text":"We will use this image . If you haven't fixed the camera in the room yet, you don't need to print it out. We will use the above image by showing on the PC display. If you have already fixed the camera, print the above image in A4 size. The size does not have to be exact. Then tape it to a cardboard box to keep it flat.","title":"Print the image for intrinsic parameter calibration"},{"location":"how-to-capture/calibrate-cameras/#print-the-image-for-extrinsic-parameter-calibration","text":"The image differs depending on the 4 methods to get extrinsic parameters explained before. Either way, the size does not have to be exact. Method using ChArUco board We will use this image . Print this in A2 or larger . You don't have a printer which can print A2? (Me too) Then, it is recommended to divide the image into two pieces, print them on two sheets of A3 paper, and tape them together. If you tape it on a cardboard like this so that it keeps in a clean flat state, you can continue to use it for a long time. \uff08The one in the photo above has been used for about 3 months, but it is still good to use.) Method using ArUco cluster We will use \"arucoMarker0.png\", \"(same)1.png\" and \"(same)2.png\" in this zip . Print them in A4 or A3. A4 is enough for a room that is not very large. Method using Diamond cluster We will use \"diamondMarker0.png\" and others in this zip . Print them in A2 or larger in the same way as \"1. Method using ChArUco board\". Method using human motion There is nothing to print.","title":"Print the image for extrinsic parameter calibration"},{"location":"how-to-capture/calibrate-cameras/#preparation-for-camera-calibration-2-measure-the-marker-size","text":"You need to measure the size of the actual printed image for extrinsic parameter calibration. The measured values will be used to define the scale of the captured movement. The part to measure differs depending on the 4 methods to get extrinsic parameters explained before. Method using ChArUco board Put the value in \"Settings > Calibration > Maker size (affects to coord. scale) > ChArUco board [m]\". The unit is meters. Method using ArUco cluster Put the value in \"Settings > Calibration > Maker size (affects to coord. scale) > ArUco marker [m]\". The unit is meters. Method using Diamond cluster Put the value in \"Settings > Calibration > Maker size (affects to coord. scale) > Diamond marker [m]\". The unit is meters. Method using human motion Measure the height of your body. Put the value in \"Settings > Calibration > Maker size (affects to coord. scale) > Human hight [m]\". The unit is meters.","title":"Preparation for camera calibration 2: Measure the marker size"},{"location":"how-to-capture/calibrate-cameras/#connect-cameras","text":"Connect at least 2 cameras to your PC. Click \"Add camera\" button at the top of the MocapForAll window. Select the combo box next to \"Camera:\" to find the connected camera. \u3000 You can change the image size of the camera by entering the dimensions and clicking \"Apply\" if camera supports the specified image size. Somtimes it fails to change the image size. In that case, please close camera, wait for a moment, and try again. You can flip the image horizontally. Note that some cameras have a mirror image by default. If the image is a mirror image, the AR marker cannot be read. You can rotate the image. At the start of capture, only a person standing upright can be recognized. So, rotate the image appropriately according to the actual orientation of the camera.","title":"Connect cameras"},{"location":"how-to-capture/calibrate-cameras/#select-camera-control-framework","text":"You can select the framework to control the camera by pressing the \"\u25bc\" next to \"Add camera\" at the top of the MocapForAll window. Direct show: Microsoft's media framework. You can use the OBS-VirtualCam plugin with this. UE4 media player: UE4's media framework. Better performance at high resolution. Some cameras don't work with this. Recorded video : Use recorded video instead of camera. See Motion capture from recorded videos . If the camera works with UE4 media player, it is recommended to use it. If it doesn't work, use Direct Show.","title":"Select camera control framework"},{"location":"how-to-capture/calibrate-cameras/#calibrate-intrinsic-parameters","text":"If you have not printed the image in Print the image for intrinsic parameter calibration section, display this image on your PC's screen as large as possible. In MocapForAll, click \"Start\" button under \"Camera > Calibration > Intrinsic\". Take images with the camera from various angles for about 10 seconds. If the camera is already fixed, move the image instead of the camera. When calibration is completed, intrinsic parameters will be displayed on the app's screen and \"Intrinsic \u2611Calibrated\" will be shown. \u3000 Save the camera calibration result from \"Save\".","title":"Calibrate intrinsic parameters"},{"location":"how-to-capture/calibrate-cameras/#calibrate-extrinsic-parameters","text":"The procedure differs depending on the 4 methods to get extrinsic parameters explained before. Method using ChArUco board Place the image printed in Print the image for extrinsic parameter calibration section on the floor. This will be the origin of the captured motion. Place the cameras where they can see the placed image. In MocapForAll, select \" ChArUco board (default) \" in \"Settings > Calibration > Extrinsic calibration method\". Click \"Start\" button under \"Camera > Calibration > Extrinsic\". When calibration is completed, extrinsic parameters and the camera position will be displayed, and it will shows \"Extrinsic \u2611Calibrated\". If it does not read the AR marker properly, try If the marker cannot be read properly . \u3000 Method using ArUco cluster Place the images printed in Print the image for extrinsic parameter calibration section on the floor. \"arucoMarker0.png\" will be the origin of the captured motion. Place the cameras where they can see the placed images. In MocapForAll, select \" ArUco cluster \" in \"Settings > Calibration > Extrinsic calibration method\". Click \" Scan markers \" button under \"Camera > Calibration > Extrinsic\". This will scan the relative positions of the markers. After a while, the markers will appear in the 3D viewport. After all markers appeared, click \" Stop scanning \" and then \" Start \" under \"Camera > Calibration > Extrinsic\". When calibration is completed, extrinsic parameters and the camera position will be displayed, and it will shows \"Extrinsic \u2611Calibrated\". If it does not read the AR marker properly, try If the marker cannot be read properly . \u3000 Method using Diamond cluster Place the images printed in Print the image for extrinsic parameter calibration section at a distance that fits in a frame of the camera. It does not have to be in the same plane. \"diamondMarker0.png\" will be the origin of the captured motion, so place this on the floor. In MocapForAll, select \" Diamond cluster \" in \"Settings > Calibration > Extrinsic calibration method\". Click \" Scan markers \" button under \"Camera > Calibration > Extrinsic\" in one of the cameras. Take pictures of \"diamondMarker0.png\" and one of the markers at the same time. After a while, the position of another marker will be fixed with \"diamondMarker0.png\" as the origin, and the marker will be displayed on the 3D viewport. Repeat the above step for markers whose positions have been fixed and markers whose positions have not been fixed, and click \"Stop scanning\" when the positions of all markers have been fixed. Place the cameras where they can see at least one of the markers whose position is fixed. Click \" Start \" under \"Camera > Calibration > Extrinsic\". When calibration is completed, extrinsic parameters and the camera position will be displayed, and it will shows \"Extrinsic \u2611Calibrated\". \u3000 Method using human motion Place the cameras so that at least two of them can see your whole body at the same time. In MocapForAll, select \" Human motion \" in \"Settings > Calibration > Extrinsic calibration method\". Select \" GPU_DirectML \" in \"Settings > General > Run DNN on\" if your hardware support it. (If you have already installed the \"GPU_TensorRT\" mode in the Appendix, you can also use it.\uff09 Select \" Speed \" in \"Settings > General > Capture body\" if your PC's performance is enough for it. (If you have already installed the \"Precision\" mode in the Appendix, you can also use it.\uff09 Click \"Start calibration\" button at the top of the window , and walk around on the floor where the cameras can see your whole body. Your motion will be captured and the positions of your joints will be used to find the cameras relative positions. After all cameras' extrinsic parameters calibrated, click the \" Find Ground \" button at the top of the window and walk around in the same way as If the marker cannot be read properly described below. This will determine the absolute position of the cameras.","title":"Calibrate extrinsic parameters"},{"location":"how-to-capture/calibrate-cameras/#if-the-marker-cannot-be-read-properly","text":"Since the position of the marker will be the plane with zero height of the captured motion, it is basically recommended to place the marker on the floor, but it may be difficult to read the marker on the floor depending on the placement and specification of the camera. In that case, you can hang the marker on the wall to calibrate the camera and adjust the floor level later . Procedure: - Replace the word \"on the floor\" with \"on the wall\" and execute the calibration procedure. - Select \" GPU_DirectML \" in \"Settings > General > Run DNN on\" if your hardware support it. (If you have already installed the \"GPU_TensorRT\" mode in the Appendix, you can also use it.\uff09 Select \" Speed \" in \"Settings > General > Capture body\" if your PC's performance is enough for it. (If you have already installed the \"Precision\" mode in the Appendix, you can also use it.\uff09 - Click the \" Find Ground \" button at the top of the window and walk around where at least 2 cameras can see your whole body. After a while, the position of the floor level will be adjusted automatically.","title":"If the marker cannot be read properly"},{"location":"how-to-capture/calibrate-cameras/#confirm-the-calibration-results","text":"If you have input the correct value in Preparation for camera calibration 2: Measure the marker size , you can see the position of the camera in the 3D viewport. For how to move the viewpoint, refer to Move viewport . Note that the camera may appear below the floor (in this case, the calibration has failed).","title":"Confirm the calibration results"},{"location":"how-to-capture/calibrate-cameras/#save-and-load-the-calibration-results","text":"After calibration is completed, save the calibration result of each camera by clicking \" Save \" under \"Camera > Calibration > Intrinsic\" and \"Extrinsic\". The saved result can be loaded by clicking \" Load \" button. Also, you can save and load the whole camera setup by clicking \" Save All Cameras \" and \" Load All Cameras \" buttons. Note that the camera selection is saved as an index inside the combo box. If you remove the cameras from your PC, the order of the cameras in the combo box will change and you will not be able to load the cameras properly.","title":"Save and load the calibration results"},{"location":"how-to-capture/get-cameras/","text":"Get cameras You need webcams that can be connected to the PC in which MocapForAll installed. To use smartphones or tablets, use apps that turns your smartphones or tablets into webcams, such as DroidCam or Iriun . What kind of camera should I use? Webcams or smartphones There is no quantitative data on which is more accurate when using a webcam or smartphone. It is best to actually plug and test your device. When using battery-powered equipment such as smartphones, be careful not to run out of battery. For your information, all official videos of MocapForAll are taken using wirelessly connected smartphone and tablet, except those that state wired cameras are used. Wired or wireless The delay of motion capture by MocapForAll mainly consists of the time it takes for the camera image to be transferred to the PC. Wired cameras often have less delay than wireless cameras. Also, keep in mind that delays tend to be large when using apps that turn smartphones into webcams or virtual cameras. To use with VR devices, it is recommended to make the delay as small as possible. FOV, frame rate, image size The wider the FOV , the wider the space for motion capture. However, MocapForAll calculates the 3D position with a simple pinhole camera model, so if the image is distorted like a fisheye lens, the movement cannot be captured correctly. A camera with low distortion and a wide FOV is most suitable. Some people use 120 degree FOV cameras, and it seems they work very well. If the frame rate is low, that value can be a bottleneck in the frame rate of the capture. Considering the performance of your PC, get cameras with a higher frame rate than that of your desired capture result. The performance of MocapForAll itself is, using GTX1080Ti for example, about 60fps when MocapForAll is operated alone, and about 20fps when used with VR at the same time. Image size does not contribute much to the precision of the capture of body movement. About 640x480 pixels is enough. This is due to the pipeline of the image processing: firstly the image is cropped around the person and then reduced to, for example, 256x256 pixel to input to the AI. However, when capturing hands and faces, the same processing is performed for small areas of hands and face, so the larger the image size (and resolution), the better the accuracy. We usually use cameras with image sizes around HD. Where should I put cameras? Put cameras where they can see your whole body as well as the following conditions are satisfied: Vertical position: chest to eye level It is recommended to place the camera at the height between your chest and your eyes. If the camera is looking up or down too much, the accuracy tends to be poor. Horizontal position: 45\u00b0 left and right from front It is recommended to place the camera at 45 degree left and right from the front of the captured person. If the capture target and the two cameras are positioned in a straight line, the accuracy tend to be poor because the depth information is insufficient. This is an example of our camera placement: Number of the cameras By increasing the number of cameras, the occlusion is reduced, so it is expected to improve the accuracy. However, there is no quantitative data on this. Since the CPU / GPU usage increases almost in proportion to the number of cameras, increasing the number of cameras will reduce the frame rate of the capture depending on the performance of your PC. We recommend that you start with two cameras and consider adding more if you find that occlusion cause problems. \uff08Examples\uff09 Capturing a person sitting in a chair with 2 cameras Capturing a person sitting in a chair with 4 cameras","title":"Get cameras"},{"location":"how-to-capture/get-cameras/#get-cameras","text":"You need webcams that can be connected to the PC in which MocapForAll installed. To use smartphones or tablets, use apps that turns your smartphones or tablets into webcams, such as DroidCam or Iriun .","title":"Get cameras"},{"location":"how-to-capture/get-cameras/#what-kind-of-camera-should-i-use","text":"","title":"What kind of camera should I use?"},{"location":"how-to-capture/get-cameras/#webcams-or-smartphones","text":"There is no quantitative data on which is more accurate when using a webcam or smartphone. It is best to actually plug and test your device. When using battery-powered equipment such as smartphones, be careful not to run out of battery. For your information, all official videos of MocapForAll are taken using wirelessly connected smartphone and tablet, except those that state wired cameras are used.","title":"Webcams or smartphones"},{"location":"how-to-capture/get-cameras/#wired-or-wireless","text":"The delay of motion capture by MocapForAll mainly consists of the time it takes for the camera image to be transferred to the PC. Wired cameras often have less delay than wireless cameras. Also, keep in mind that delays tend to be large when using apps that turn smartphones into webcams or virtual cameras. To use with VR devices, it is recommended to make the delay as small as possible.","title":"Wired or wireless"},{"location":"how-to-capture/get-cameras/#fov-frame-rate-image-size","text":"The wider the FOV , the wider the space for motion capture. However, MocapForAll calculates the 3D position with a simple pinhole camera model, so if the image is distorted like a fisheye lens, the movement cannot be captured correctly. A camera with low distortion and a wide FOV is most suitable. Some people use 120 degree FOV cameras, and it seems they work very well. If the frame rate is low, that value can be a bottleneck in the frame rate of the capture. Considering the performance of your PC, get cameras with a higher frame rate than that of your desired capture result. The performance of MocapForAll itself is, using GTX1080Ti for example, about 60fps when MocapForAll is operated alone, and about 20fps when used with VR at the same time. Image size does not contribute much to the precision of the capture of body movement. About 640x480 pixels is enough. This is due to the pipeline of the image processing: firstly the image is cropped around the person and then reduced to, for example, 256x256 pixel to input to the AI. However, when capturing hands and faces, the same processing is performed for small areas of hands and face, so the larger the image size (and resolution), the better the accuracy. We usually use cameras with image sizes around HD.","title":"FOV, frame rate, image size"},{"location":"how-to-capture/get-cameras/#where-should-i-put-cameras","text":"Put cameras where they can see your whole body as well as the following conditions are satisfied:","title":"Where should I put cameras?"},{"location":"how-to-capture/get-cameras/#vertical-position-chest-to-eye-level","text":"It is recommended to place the camera at the height between your chest and your eyes. If the camera is looking up or down too much, the accuracy tends to be poor.","title":"Vertical position: chest to eye level"},{"location":"how-to-capture/get-cameras/#horizontal-position-45-left-and-right-from-front","text":"It is recommended to place the camera at 45 degree left and right from the front of the captured person. If the capture target and the two cameras are positioned in a straight line, the accuracy tend to be poor because the depth information is insufficient. This is an example of our camera placement:","title":"Horizontal position: 45\u00b0 left and right from front"},{"location":"how-to-capture/get-cameras/#number-of-the-cameras","text":"By increasing the number of cameras, the occlusion is reduced, so it is expected to improve the accuracy. However, there is no quantitative data on this. Since the CPU / GPU usage increases almost in proportion to the number of cameras, increasing the number of cameras will reduce the frame rate of the capture depending on the performance of your PC. We recommend that you start with two cameras and consider adding more if you find that occlusion cause problems. \uff08Examples\uff09 Capturing a person sitting in a chair with 2 cameras Capturing a person sitting in a chair with 4 cameras","title":"Number of the cameras"},{"location":"how-to-capture/settings-on-the-app/","text":"Set up the app","title":"Settings on the app"},{"location":"how-to-export/","text":"","title":"How to export motion"},{"location":"how-to-install/","text":"","title":"How to install"},{"location":"how-to-install/from-booth/","text":"Install and update from BOOTH How to download Free trial version Free trial version is available on BOOTH. BOOTH store It is required to try free version and confirm that the software works without problems in your environment before purchase of paid version. The free version has limited functionality only for data export. - Data sending via VMT protocol and VMC protocol stops and restarts every 10 seconds - Maximum frames in a BVH file is limited to 300 Paid version Before purchase, please read and accept the terms and conditions from the following link. If you agree, you can get your purchase password and proceed to the purchase page at BOOTH. Our HP You need a pixiv account to purchase at BOOTH. How to install You can install MocapForAll manually or by using network installer from BOOTH. Manual installation Main files Download and unzip \"MocapForAll_Full_vN.N.N.zip\" Execute MocapForAll.exe in \"MocapForAll_Full_vN.N.N\" folder If the \"UE4 Prerequisites\" installation screen is displayed, install it. Appendix (Optional) If you do not need the functions in Appendix, you can skip this step. There are four Appendix as follows: Appendix1\uff1aPrecision mode Adds \"Precision\" mode for precise motion capture. Since the CPU / GPU usage is very high, it is not recommended to use with VR apps at the same time. Appendix2\uff1aHDRI maps Adds maps which use HDRI images from HDRI Haven . Appendix3\uff1aMetaHuman character Adds a character created by Epic games' MetaHuman Creator. Used to test motion capture on MocapForAll. Appendix4\uff1aTensorRT mode (Deprecated) Adds \"GPU_TensorRT\" mode which provides GPU acceleration on supported NVIDIA GPUs. As described below, CUDA, cuDNN, and TensorRT need to be installed. (In most cases, the standard \"GPU_DirectML\" mode will suffice.) This feature is deprecated because the improvement of performance is often very small or sometimes negative, and whether it works depends on the user's environment. How to install Appendix Download and unzip \"AppendixN_xxxxx_yyyyy.zip\". Overwrite \"MocapForAll_Full_vN.N.N\\MocapForAll\" with \"AppendixN_xxxxx_yyyyy\\MocapForAll\". If you install \"Appendix4_TensorRT_mode\", follow the installation guide of TensorRT . Installation by Network Installer Download and unzip \"Network_Installer_-_MocapForAll_Full_vN.N.N.zip\". Execute \"Network_Installer_-_MocapForAll_Full_vN.N.N.exe\". Select the required Appendix. See Appendix for the contents. To use \"Appendix4_TensorRT_mode\", see Installation of TensorRT and install the required software. Run MocapForAll from the Start menu or MocapForAll.exe in the installation path. If the \"UE4 Prerequisites\" installation screen is displayed, install it. How to update from BOOTH Manual update Download and unzip \"MocapForAll_Full_vN.N.N.zip\". Overwrite the old version of \"MocapForAll_Full_vM.M.M\" with new \"MocapForAll_Full_vN.N.N\". Update by Network Installer Same as installation. If you wan to reduce the data size to download, select only \"Main Files\" in \"Select Components\" screen without selecting \"Appendix\" and execute installation. Since the installer does not delete files, the previous Appendix remains . After that, Appendix will be treated as not installed on the \"Select Components\" screen of the installer, but this cause no problem.","title":"From BOOTH"},{"location":"how-to-install/from-booth/#install-and-update-from-booth","text":"","title":"Install and update from BOOTH"},{"location":"how-to-install/from-booth/#how-to-download","text":"","title":"How to download"},{"location":"how-to-install/from-booth/#free-trial-version","text":"Free trial version is available on BOOTH. BOOTH store It is required to try free version and confirm that the software works without problems in your environment before purchase of paid version. The free version has limited functionality only for data export. - Data sending via VMT protocol and VMC protocol stops and restarts every 10 seconds - Maximum frames in a BVH file is limited to 300","title":"Free trial version"},{"location":"how-to-install/from-booth/#paid-version","text":"Before purchase, please read and accept the terms and conditions from the following link. If you agree, you can get your purchase password and proceed to the purchase page at BOOTH. Our HP You need a pixiv account to purchase at BOOTH.","title":"Paid version"},{"location":"how-to-install/from-booth/#how-to-install","text":"You can install MocapForAll manually or by using network installer from BOOTH.","title":"How to install"},{"location":"how-to-install/from-booth/#manual-installation","text":"","title":"Manual installation"},{"location":"how-to-install/from-booth/#main-files","text":"Download and unzip \"MocapForAll_Full_vN.N.N.zip\" Execute MocapForAll.exe in \"MocapForAll_Full_vN.N.N\" folder If the \"UE4 Prerequisites\" installation screen is displayed, install it.","title":"Main files"},{"location":"how-to-install/from-booth/#appendix-optional","text":"If you do not need the functions in Appendix, you can skip this step. There are four Appendix as follows: Appendix1\uff1aPrecision mode Adds \"Precision\" mode for precise motion capture. Since the CPU / GPU usage is very high, it is not recommended to use with VR apps at the same time. Appendix2\uff1aHDRI maps Adds maps which use HDRI images from HDRI Haven . Appendix3\uff1aMetaHuman character Adds a character created by Epic games' MetaHuman Creator. Used to test motion capture on MocapForAll. Appendix4\uff1aTensorRT mode (Deprecated) Adds \"GPU_TensorRT\" mode which provides GPU acceleration on supported NVIDIA GPUs. As described below, CUDA, cuDNN, and TensorRT need to be installed. (In most cases, the standard \"GPU_DirectML\" mode will suffice.) This feature is deprecated because the improvement of performance is often very small or sometimes negative, and whether it works depends on the user's environment.","title":"Appendix (Optional)"},{"location":"how-to-install/from-booth/#how-to-install-appendix","text":"Download and unzip \"AppendixN_xxxxx_yyyyy.zip\". Overwrite \"MocapForAll_Full_vN.N.N\\MocapForAll\" with \"AppendixN_xxxxx_yyyyy\\MocapForAll\". If you install \"Appendix4_TensorRT_mode\", follow the installation guide of TensorRT .","title":"How to install Appendix"},{"location":"how-to-install/from-booth/#installation-by-network-installer","text":"Download and unzip \"Network_Installer_-_MocapForAll_Full_vN.N.N.zip\". Execute \"Network_Installer_-_MocapForAll_Full_vN.N.N.exe\". Select the required Appendix. See Appendix for the contents. To use \"Appendix4_TensorRT_mode\", see Installation of TensorRT and install the required software. Run MocapForAll from the Start menu or MocapForAll.exe in the installation path. If the \"UE4 Prerequisites\" installation screen is displayed, install it.","title":"Installation by Network Installer"},{"location":"how-to-install/from-booth/#how-to-update-from-booth","text":"","title":"How to update from BOOTH"},{"location":"how-to-install/from-booth/#manual-update","text":"Download and unzip \"MocapForAll_Full_vN.N.N.zip\". Overwrite the old version of \"MocapForAll_Full_vM.M.M\" with new \"MocapForAll_Full_vN.N.N\".","title":"Manual update"},{"location":"how-to-install/from-booth/#update-by-network-installer","text":"Same as installation. If you wan to reduce the data size to download, select only \"Main Files\" in \"Select Components\" screen without selecting \"Appendix\" and execute installation. Since the installer does not delete files, the previous Appendix remains . After that, Appendix will be treated as not installed on the \"Select Components\" screen of the installer, but this cause no problem.","title":"Update by Network Installer"},{"location":"how-to-install/from-steam/","text":"Install and update from Steam Get from Steam Steam store Before purchase, - Please read and accept the terms and conditions from this link . - Please try Demo version to check if the app works in your environment. Demo version has limited functionality only for data export. - Data sending via VMT protocol and VMC protocol stops and restarts every 10 seconds - Maximum frames in a BVH file is limited to 300 How to install and update Main program Just use Steam client app as usual. Appendix All Appendix are installed automatically. If you want to use GPU acceleration by TensorRT, you need to manually install CUDA, cuDNN, and TensorRT .","title":"From Steam"},{"location":"how-to-install/from-steam/#install-and-update-from-steam","text":"","title":"Install and update from Steam"},{"location":"how-to-install/from-steam/#get-from-steam","text":"Steam store Before purchase, - Please read and accept the terms and conditions from this link . - Please try Demo version to check if the app works in your environment. Demo version has limited functionality only for data export. - Data sending via VMT protocol and VMC protocol stops and restarts every 10 seconds - Maximum frames in a BVH file is limited to 300","title":"Get from Steam"},{"location":"how-to-install/from-steam/#how-to-install-and-update","text":"","title":"How to install and update"},{"location":"how-to-install/from-steam/#main-program","text":"Just use Steam client app as usual.","title":"Main program"},{"location":"how-to-install/from-steam/#appendix","text":"All Appendix are installed automatically. If you want to use GPU acceleration by TensorRT, you need to manually install CUDA, cuDNN, and TensorRT .","title":"Appendix"},{"location":"how-to-install/install-tensorrt/","text":"Install TensorRT This section is only for enthusiasts who want to improve performance even by 1ms/frame. \uff08Using GTX1080Ti in our dev env, TensorRT has better performance than DirectML by 1ms/frame/camera in Precision mode, for example.) Requires an Nvidia GPU that supports CUDA, cuDNN, and TensorRT. Please note that the versions of cuDNN and TensorRT are different for RTX30** series and others as shown below (*3). If you are using RTX2080Ti, you might need to use cuDNN v8.0.5. Please try various combinations of the followings. For the RTX30 ** series, we only tested RTX3060Ti and RTX3070. Others are not tested. Other than RTX30** series RTX30** series CUDA 11.0.3 11.0.3 cuDNN v8.0.2 (July 24th, 2020), for CUDA 11.0 v8.0.5 (November 9th, 2020), for CUDA 11.0 TensorRT 7.1.3.4 for CUDA 11.0 7.2.2.3 for CUDA 11.0 (*3: The versions for RTX30** series were provided by \u6f06\u539f \u938c\u8db3 san. Thank you!) From now on, we will only explain the case of other than RTX30** series . If you are using RTX30** series, please read the versions as appropriate. Install CUDA 11.0.3 Please note that the installer may fail to install the NVIDIA driver. In that case, please install the latest NVIDIA driver manually. Download and unzip cuDNN v8.0.2 (July 24th, 2020), for CUDA 11.0 Overwrite the following folders C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\include C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\lib with unzipped cudnn-11.0-windows-x64-v8.0.2.39\\cuda\\bin cudnn-11.0-windows-x64-v8.0.2.39\\cuda\\include cudnn-11.0-windows-x64-v8.0.2.39\\cuda\\lib Add an environment variable \"CUDNN_PATH\", and set its value to \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\". Download and unzip TensorRT7.1.3.4 Note that a Nvidia account is required. Add the path of lib folder in that to the environment variable \"PATH\", for example \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\TensorRT-7.1.3.4\\lib\" Add an environment variable \"ORT_TENSORRT_ENGINE_CACHE_ENABLE\" and set its value to \"1\". Add an environment variable \"ORT_TENSORRT_CACHE_PATH\" and set its value to any path where you want to save the cache files, for example \"C:\\temp\". \uff08(For other options of TensorRT, see this \uff09 \u3000 \uff08Your environment variables will be something like this\uff09 \u200b \uff08Your environment variable \"Path\" will be something like this\uff09","title":"Install TensorRT"},{"location":"how-to-install/install-tensorrt/#install-tensorrt","text":"This section is only for enthusiasts who want to improve performance even by 1ms/frame. \uff08Using GTX1080Ti in our dev env, TensorRT has better performance than DirectML by 1ms/frame/camera in Precision mode, for example.) Requires an Nvidia GPU that supports CUDA, cuDNN, and TensorRT. Please note that the versions of cuDNN and TensorRT are different for RTX30** series and others as shown below (*3). If you are using RTX2080Ti, you might need to use cuDNN v8.0.5. Please try various combinations of the followings. For the RTX30 ** series, we only tested RTX3060Ti and RTX3070. Others are not tested. Other than RTX30** series RTX30** series CUDA 11.0.3 11.0.3 cuDNN v8.0.2 (July 24th, 2020), for CUDA 11.0 v8.0.5 (November 9th, 2020), for CUDA 11.0 TensorRT 7.1.3.4 for CUDA 11.0 7.2.2.3 for CUDA 11.0 (*3: The versions for RTX30** series were provided by \u6f06\u539f \u938c\u8db3 san. Thank you!) From now on, we will only explain the case of other than RTX30** series . If you are using RTX30** series, please read the versions as appropriate. Install CUDA 11.0.3 Please note that the installer may fail to install the NVIDIA driver. In that case, please install the latest NVIDIA driver manually. Download and unzip cuDNN v8.0.2 (July 24th, 2020), for CUDA 11.0 Overwrite the following folders C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\include C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\lib with unzipped cudnn-11.0-windows-x64-v8.0.2.39\\cuda\\bin cudnn-11.0-windows-x64-v8.0.2.39\\cuda\\include cudnn-11.0-windows-x64-v8.0.2.39\\cuda\\lib Add an environment variable \"CUDNN_PATH\", and set its value to \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\". Download and unzip TensorRT7.1.3.4 Note that a Nvidia account is required. Add the path of lib folder in that to the environment variable \"PATH\", for example \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\TensorRT-7.1.3.4\\lib\" Add an environment variable \"ORT_TENSORRT_ENGINE_CACHE_ENABLE\" and set its value to \"1\". Add an environment variable \"ORT_TENSORRT_CACHE_PATH\" and set its value to any path where you want to save the cache files, for example \"C:\\temp\". \uff08(For other options of TensorRT, see this \uff09 \u3000 \uff08Your environment variables will be something like this\uff09 \u200b \uff08Your environment variable \"Path\" will be something like this\uff09","title":"Install TensorRT"},{"location":"other-settings/","text":"","title":"Other settings"},{"location":"other-settings/capture-hand-and-face/","text":"Capture hand and face Turn on \"Settings > General > Capture hand\" to capture finger movements Turn on \"Settings > General > Capture face\" to capture facial expressions Note that these are experimental features. The accuracy is low and CPU / GPU usage is high. Cropping size for hand / face You can adjust the crop size of the image used to capture your hands and face from \"Settings > Advanced > Cropping size for hand\" and \"Cropping size for face\". Since different people have different hand and face sizes, adjusting these values may improve accuracy. Hand capture Full / Lite You can select the hand tracking model from Full, Lite or Legacy from the pulldown \"Settings > General > Capture hand\". \"Full\": More accurate. Uses more CPU/GPU. \"Lite\": Less accurate. Uses less CPU/GPU. \"Legacy\": Uses the model used in v1.13 and before. For backward compatibility. Specify facial morph target names You can specify the names of facial morph targets if you are using VRM models. Turn on \"Settings > Advanced > Specify facial morph target names\" Input morph target names","title":"Capture hand and face"},{"location":"other-settings/capture-hand-and-face/#capture-hand-and-face","text":"Turn on \"Settings > General > Capture hand\" to capture finger movements Turn on \"Settings > General > Capture face\" to capture facial expressions Note that these are experimental features. The accuracy is low and CPU / GPU usage is high.","title":"Capture hand and face"},{"location":"other-settings/capture-hand-and-face/#cropping-size-for-hand-face","text":"You can adjust the crop size of the image used to capture your hands and face from \"Settings > Advanced > Cropping size for hand\" and \"Cropping size for face\". Since different people have different hand and face sizes, adjusting these values may improve accuracy.","title":"Cropping size for hand / face"},{"location":"other-settings/capture-hand-and-face/#hand-capture-full-lite","text":"You can select the hand tracking model from Full, Lite or Legacy from the pulldown \"Settings > General > Capture hand\". \"Full\": More accurate. Uses more CPU/GPU. \"Lite\": Less accurate. Uses less CPU/GPU. \"Legacy\": Uses the model used in v1.13 and before. For backward compatibility.","title":"Hand capture Full / Lite"},{"location":"other-settings/capture-hand-and-face/#specify-facial-morph-target-names","text":"You can specify the names of facial morph targets if you are using VRM models. Turn on \"Settings > Advanced > Specify facial morph target names\" Input morph target names","title":"Specify facial morph target names"}]}